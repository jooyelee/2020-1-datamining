{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 brest cancer wisconsin data를 통해서 의사결정을 도와줄 수 있는 효과적인 classification model을 만드는 것이 목적이다\n",
    "\n",
    "그렇기 떄문에 다양한 데이터 전처리: data transformation, feature selection 을 이용하고 다양한 decision model algorithm을 이용해 여러가지 모델들의 accuracy를 비교하고 그중 가장 성능이 좋은 모델을 채택할 것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 사용하는 데이터에 대해 간단한 설명을 하자면 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#```python\n",
    "import pandas as pd                                                                                                                                                                                                                                                          \n",
    "table = pd.read_csv(\"wdbc.csv\",header = None)\n",
    "name =[\"ID\", \"diagnosis\",\"radius\",\"texture\",\"perimeter\",\"area\",\"smoothness\",\"compactness\",\"concavity\",\"concave points\",\"symmetry\",\"fractal dimention\",\n",
    "\"radius_SD\",\"texture_SD\",\"perimeter_SD\",\"area_SD\",\"smoothness_SD\",\"compactness_SD\",\"concavity_SD\",\"concave points_SD\",\"symmetry_SD\",\"fractal dimention_SD\",\n",
    "\"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\"concavity_worst\",\"concave points_worst\",\"symmetry_worst\",\"fractal dimention_worst\"\n",
    " ]\n",
    "table.columns = name\n",
    "#```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data pre-processing\n",
    "\n",
    "1. check missing value\n",
    "\n",
    "2. select attribute: id 제거\n",
    "\n",
    "3. box plot 을 통한 data scale(SD, mean) 평가\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "table.columns = name\n",
    "# null 값 찾기\n",
    "pd.isnull(table).any()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "t_a = table.iloc[:,1:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import seaborn as sns\n",
    "sns.countplot(table[\"diagnosis\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 plot을 보면 이 데이터의 클라스 분포를 알수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "t_a.boxplot(column=name[2:12],figsize=(18,5))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "t_a.boxplot(column=name[12:22],figsize=(18,5))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "t_a.boxplot(column=name[22:32],figsize=(18,5))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data transformation\n",
    "\n",
    "##### data transformation to \n",
    "1. mean = 0, sd = 1\n",
    "2. min_max scale [0,1]\n",
    "3. max_abs scale [-1,1]\n",
    "4. normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# data set attribute / class로 나누기\n",
    "x_ori = t_a.iloc[:,1:]\n",
    "y_ori = t_a.iloc[:,0]\n",
    "\n",
    "# data test set, train set으로 분리\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_ori,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "# scale function mean=0, sd = 1\n",
    "scaler = preprocessing.StandardScaler().fit(x_ori)\n",
    "scale_x_test = scaler.transform(x_test)\n",
    "scale_x_train = scaler.transform(x_train)\n",
    "scale_x = scaler.transform(x_ori)\n",
    "\n",
    "\n",
    "# min_max_scaler  [0,1]\n",
    "min_max_scaler = preprocessing.MinMaxScaler().fit(x_ori)\n",
    "min_max_x_test = min_max_scaler.transform(x_test)\n",
    "min_max_x_train= min_max_scaler.transform(x_train)\n",
    "min_max_x = min_max_scaler.transform(x_ori)\n",
    "\n",
    "# max_abs_scaler\n",
    "max_abs_scaler=preprocessing.MaxAbsScaler().fit(x_ori)\n",
    "max_abs_x_test = max_abs_scaler.transform(x_test)\n",
    "max_abs_x_train= max_abs_scaler.transform(x_train)\n",
    "max_abs_x = max_abs_scaler.transform(x_ori)\n",
    "\n",
    "# normalization\n",
    "normalization=preprocessing.Normalizer().fit(x_ori)\n",
    "normalization_x_test = normalization.transform(x_test)\n",
    "normalization_x_train= normalization.transform(x_train)\n",
    "normalization_x = normalization.transform(x_ori)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for feature selection\n",
    "\n",
    "1. draw a correaltion matrix in order to find dependency among attributes\n",
    "2. do a PCA with all test sets ( scale, maxmin, maxabs, norm ) and select a number of pca principle component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "corr4 = t_a.corr()\n",
    "sns.heatmap(corr4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 만든 correlation matrix로 직접 feature selection을 하려고 했으니 어느 정도 상관관계 까지 지워야 할지 확실한 기준이 없기 때문에, wrapper apporach를 통해 나중에 비중있게 사용된 attribute을 고려 할 것이다.\n",
    "\n",
    "이 프로젝트의 목적은 예측력 높은 모델을 생성하는 것이 목적이라고 말했는데, 단순하게 scaled된 데이터를 가지고 하는 것 뿐만 아니라\n",
    "PCA를 통해 새로운 feature를 생성해내서 그를 가지고 model을 생성할 것이다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "## normal\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "########## normal #############################\n",
    "pca = list()\n",
    "pca_explained_variance_ratio = list()\n",
    "\n",
    "for i in range(1,31):\n",
    "    scale = PCA(n_components=i)\n",
    "    scale.fit_transform(x_ori)\n",
    "    pca.append(scale)\n",
    "    pca_explained_variance_ratio.append(scale.explained_variance_ratio_.tolist())\n",
    "    \n",
    "    \n",
    "pca_ratio = pd.DataFrame(pca_explained_variance_ratio)   \n",
    "\n",
    "toplot_norm = pca_ratio.T.sum()\n",
    "print(toplot_norm[toplot_norm>0.95].head(1))\n",
    "\n",
    "toplot_norm.plot(title = \"explained_variance_ratio normal \",grid=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "## scale\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = list()\n",
    "pca_explained_variance_ratio = list()\n",
    "\n",
    "for i in range(1,31):\n",
    "    scale = PCA(n_components=i)\n",
    "    scale.fit_transform(scale_x)\n",
    "    pca.append(scale)\n",
    "    pca_explained_variance_ratio.append(scale.explained_variance_ratio_.tolist())\n",
    "    \n",
    "    \n",
    "pca_ratio = pd.DataFrame(pca_explained_variance_ratio)   \n",
    "\n",
    "toplot_scale = pca_ratio.T.sum()\n",
    "print(toplot_scale[toplot_scale>0.95].head(2))\n",
    "toplot_scale.plot(title = \"explained_variance_ratio scaled\",grid=True)\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "## min max\n",
    "```python\n",
    "\n",
    "pca = list()\n",
    "pca_explained_variance_ratio = list()\n",
    "\n",
    "for i in range(1,31):\n",
    "    minmax = PCA(n_components=i)\n",
    "    minmax.fit_transform(min_max_x)\n",
    "    pca.append(minmax)\n",
    "    pca_explained_variance_ratio.append(minmax.explained_variance_ratio_.tolist())\n",
    "pca_ratio = pd.DataFrame(pca_explained_variance_ratio)    \n",
    "\n",
    "toplot_minmax = pca_ratio.T.sum()\n",
    "print(toplot_minmax[toplot_minmax>0.95].head(2))\n",
    "toplot_minmax.plot(title = \"explained_variance_ratio minmax\",grid=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "## max abs\n",
    "```python\n",
    "pca = list()\n",
    "pca_explained_variance_ratio = list()\n",
    "\n",
    "for i in range(1,31):\n",
    "    maxabs = PCA(n_components=i)\n",
    "    maxabs.fit_transform(max_abs_x)\n",
    "    pca.append(maxabs)\n",
    "    pca_explained_variance_ratio.append(maxabs.explained_variance_ratio_.tolist())\n",
    "pca_ratio = pd.DataFrame(pca_explained_variance_ratio)    \n",
    "\n",
    "toplot_maxabs = pca_ratio.T.sum()\n",
    "print(toplot_maxabs[toplot_maxabs>0.95].head(2))\n",
    "toplot_maxabs.plot(title = \"explained_variance_ratio maxabs\",grid=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "## normal\n",
    "```python\n",
    "pca = list()\n",
    "pca_explained_variance_ratio = list()\n",
    "\n",
    "for i in range(1,31):\n",
    "    norm = PCA(n_components=i)\n",
    "    norm.fit_transform(normalization_x)\n",
    "    pca.append(norm)\n",
    "    pca_explained_variance_ratio.append(norm.explained_variance_ratio_.tolist())\n",
    "pca_ratio = pd.DataFrame(pca_explained_variance_ratio)    \n",
    "\n",
    "toplot_norm = pca_ratio.T.sum()\n",
    "print(toplot_norm[toplot_norm>0.95].head(2))\n",
    "toplot_norm.plot(title = \"explained_variance_ratio norm \",grid=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate PCA of which explaned sum of varience is just bigger than 0.95\n",
    "\n",
    "```python\n",
    "k_n = 1\n",
    "k = PCA(n_components=k_n)\n",
    "k= k.fit_transform(x_ori)\n",
    "pca_norm= pd.DataFrame(data=k,columns = [\"comp\"+str(i+1) for i in range(k_n)] )\n",
    "pca_norm_ = pd.concat([pca_norm,y_ori],axis=1)\n",
    "\n",
    "k_n = 9\n",
    "k = PCA(n_components=k_n)\n",
    "k= k.fit_transform(scale_x)\n",
    "pca_scale= pd.DataFrame(data=k,columns =[\"comp\"+str(i+1) for i in range(k_n)] )\n",
    "pca_scale_ = pd.concat([pca_scale,y_ori],axis=1)\n",
    "\n",
    "k_n = 9\n",
    "k = PCA(n_components=k_n)\n",
    "k=k.fit_transform(min_max_x)\n",
    "pca_minmax= pd.DataFrame(data=k,columns = [\"comp\"+str(i+1) for i in range(k_n)] )\n",
    "pca_minmax_ = pd.concat([pca_minmax,y_ori],axis=1)\n",
    "\n",
    "k_n = 8\n",
    "k = PCA(n_components=k_n)\n",
    "k=k.fit_transform(max_abs_x)\n",
    "pca_maxabs= pd.DataFrame(data=k,columns = [\"comp\"+str(i+1) for i in range(k_n)] )\n",
    "pca_maxabs_ = pd.concat([pca_maxabs,y_ori],axis=1)\n",
    "\n",
    "k_n = 2\n",
    "k = PCA(n_components=k_n)\n",
    "k=k.fit_transform(max_abs_x)\n",
    "pca_normal= pd.DataFrame(data=k,columns = ['comp1','comp2'] )\n",
    "pca_normal_ = pd.concat([pca_norm,y_ori],axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금 까지 data preprocessing 으로 mead = 0, sd =1 / min max / max abs / normalization 에 해당하는 데이터 셋을 따로 만들었고, \n",
    "feature selection으로 ID attribute를 없엤고,\n",
    "feature creation의 일종인 PCA를 통해 새로운 table을 만들었다.\n",
    "\n",
    "이제 이를 이용하여 다양한 classifier 를 만들것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decision tree 와 random forest를 우선 사용할 것인데 이 모델에서는 예측력을 높이는 것도 목적이지만, 모델을 해석하여 어떤 attribute가 영양을 많이 끼치는지 알아보기 위해서 PCA는 사용하지 않을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision tree\n",
    "\n",
    "### original data\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "decisionTree_entropy = []\n",
    "decisionTree_gini = []\n",
    "\n",
    "#### 트리 생성_entropy\n",
    "diagnosis_tree_entropy = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state = 0)\n",
    "diagnosis_tree_entropy.fit(x_train,y_train)\n",
    "\n",
    "#### 트리 생성_gini\n",
    "diagnosis_tree_gini = tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 0)\n",
    "diagnosis_tree_gini.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "#### accuracy 구하기\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#### entropy\n",
    "dia_predict = diagnosis_tree_entropy.predict(x_test)\n",
    "print('Accuracy of \"normal\" by criterion = entropy: %.4f' % accuracy_score(y_test, dia_predict))\n",
    "decisionTree_entropy.append(accuracy_score(y_test, dia_predict))\n",
    "\n",
    "#### gini\n",
    "dia_predict = diagnosis_tree_gini.predict(x_test)\n",
    "print('Accuracy of \"normal\" by criterion = gini: %.4f' % accuracy_score(y_test, dia_predict))\n",
    "decisionTree_gini.append(accuracy_score(y_test, dia_predict))\n",
    "```\n",
    "### scaled data\n",
    "\n",
    "```python\n",
    "#############################  mean = 0, sd = 1 data################################\n",
    "\n",
    "x_train_ = scale_x_train\n",
    "x_test_ = scale_x_test\n",
    "\n",
    "# 트리 생성_entropy\n",
    "diagnosis_tree_entropy1 = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state = 0)\n",
    "diagnosis_tree_entropy1.fit(x_train_,y_train)\n",
    "\n",
    "# 트리 생성_gini\n",
    "diagnosis_tree_gini1 = tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 0)\n",
    "diagnosis_tree_gini1.fit(x_train_,y_train)\n",
    "\n",
    "\n",
    "# accuracy 구하기\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# entropy\n",
    "dia_predict = diagnosis_tree_entropy1.predict(x_test_)\n",
    "print('Accuracy of \"scale\" by criterion = entropy: %.4f' % accuracy_score(y_test, dia_predict))\n",
    "decisionTree_entropy.append(accuracy_score(y_test, dia_predict))\n",
    "\n",
    "# gini\n",
    "dia_predict = diagnosis_tree_gini1.predict(x_test_)\n",
    "print('Accuracy of \"scale\" by criterion = gini: %.4f' % accuracy_score(y_test, dia_predict))\n",
    "decisionTree_gini.append(accuracy_score(y_test, dia_predict))\n",
    "```\n",
    "###  min maxed data\n",
    "```python\n",
    "######################  min max  [0,1]  data #############################\n",
    "\n",
    "x_train_ = min_max_x_train\n",
    "x_test_ = min_max_x_test\n",
    "\n",
    "# 트리 생성_entropy\n",
    "diagnosis_tree_entropy11 = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state = 0)\n",
    "diagnosis_tree_entropy11.fit(x_train_,y_train)\n",
    "\n",
    "# 트리 생성_gini\n",
    "diagnosis_tree_gini11 = tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 0)\n",
    "diagnosis_tree_gini11.fit(x_train_,y_train)\n",
    "\n",
    "\n",
    "# accuracy 구하기\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# entropy\n",
    "dia_predict = diagnosis_tree_entropy11.predict(x_test_)\n",
    "print('Accuracy of \"minmax\" by criterion = entropy: %.4f' % accuracy_score(y_test, dia_predict))\n",
    "decisionTree_entropy.append(accuracy_score(y_test, dia_predict))\n",
    "\n",
    "# gini\n",
    "dia_predict = diagnosis_tree_gini11.predict(x_test_)\n",
    "print('Accuracy of \"minmax\" by criterion = gini: %.4f' % accuracy_score(y_test, dia_predict))\n",
    "decisionTree_gini.append(accuracy_score(y_test, dia_predict))\n",
    "```\n",
    "### max abs data\n",
    "```python\n",
    "#################### max abs [-1,1]  data  #####################################\n",
    "\n",
    "x_train_ = max_abs_x_train\n",
    "x_test_ = max_abs_x_test\n",
    "\n",
    "# 트리 생성_entropy\n",
    "diagnosis_tree_entropy12 = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state = 0)\n",
    "diagnosis_tree_entropy12.fit(x_train_,y_train)\n",
    "\n",
    "# 트리 생성_gini\n",
    "diagnosis_tree_gini12 = tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 0)\n",
    "diagnosis_tree_gini12.fit(x_train_,y_train)\n",
    "\n",
    "\n",
    "# accuracy 구하기\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# entropy\n",
    "dia_predict = diagnosis_tree_entropy12.predict(x_test_)\n",
    "print('Accuracy of \"max abs\" by criterion = entropy: %.4f' % accuracy_score(y_test, dia_predict))\n",
    "decisionTree_entropy.append(accuracy_score(y_test, dia_predict))\n",
    "\n",
    "# gini\n",
    "dia_predict = diagnosis_tree_gini12.predict(x_test_)\n",
    "print('Accuracy of \"max abs\" by criterion = gini: %.4f' % accuracy_score(y_test, dia_predict))\n",
    "decisionTree_gini.append(accuracy_score(y_test, dia_predict))\n",
    "```\n",
    "### normalized data\n",
    "```python\n",
    "####################### normalization data #########################################\n",
    "\n",
    "x_train_ = normalization_x_train\n",
    "x_test_ = normalization_x_test\n",
    "\n",
    "# 트리 생성_entropy\n",
    "diagnosis_tree_entropy13 = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state = 0)\n",
    "diagnosis_tree_entropy13.fit(x_train_,y_train)\n",
    "\n",
    "# 트리 생성_gini\n",
    "diagnosis_tree_gini13 = tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 0)\n",
    "diagnosis_tree_gini13.fit(x_train_,y_train)\n",
    "\n",
    "\n",
    "# accuracy 구하기\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# entropy\n",
    "dia_predict = diagnosis_tree_entropy13.predict(x_test_)\n",
    "print('Accuracy of \"normalization\" by criterion = entropy: %.4f' % accuracy_score(y_test, dia_predict))\n",
    "decisionTree_entropy.append(accuracy_score(y_test, dia_predict))\n",
    "\n",
    "# gini\n",
    "dia_predict = diagnosis_tree_gini13.predict(x_test_)\n",
    "print('Accuracy of \"normalization\" by criterion = gini: %.4f' % accuracy_score(y_test, dia_predict))\n",
    "decisionTree_gini.append(accuracy_score(y_test, dia_predict))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree_acc = pd.DataFrame()\n",
    "\n",
    "decisionTree_acc[\"gini\"] =decisionTree_gini\n",
    "decisionTree_acc[\"entropy\"] = decisionTree_entropy \n",
    "\n",
    "decisionTree_acc.index = [\"ori\",\"scale\",\"minmax\",\"max abs\",\"normalize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "# entropy decision tree 시각화\n",
    "dot_data = export_graphviz(diagnosis_tree_entropy1, out_file = None, feature_names = name[2:], class_names = [\"benign\",\"malignant\"],filled = True,rounded=True,special_characters = True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee Joo Ye\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of \"normal Random Forest\" by criterion = entropy: 0.9649\n",
      "Accuracy of \"normal Random Forest\" by criterion = gini: 0.9649\n",
      "Accuracy of \"scaled Random Forest\" by criterion = entropy: 0.9649\n",
      "Accuracy of \"scaled Random Forest\" by criterion = gini: 0.9649\n",
      "Accuracy of \"min max Random Forest\" by criterion = entropy: 0.9649\n",
      "Accuracy of \"min max Random Forest\" by criterion = gini: 0.9649\n",
      "Accuracy of \"max abs Random Forest\" by criterion = entropy: 0.9649\n",
      "Accuracy of \"max abs Random Forest\" by criterion = gini: 0.9649\n",
      "Accuracy of \"normalization Random Forest\" by criterion = entropy: 0.9415\n",
      "Accuracy of \"normalization Random Forest\" by criterion = gini: 0.9591\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randomForest_gini = []\n",
    "randomForest_entropy = []\n",
    "\n",
    "###################### normal data #################################\n",
    "\n",
    "\n",
    "# random forest 생성_gini\n",
    "rf_gini = RandomForestClassifier(random_state = 0).fit(x_train,y_train.values.ravel())\n",
    "# random forest_entropy\n",
    "rf_entropy = RandomForestClassifier(criterion=\"entropy\" ,random_state = 0).fit(x_train,y_train.values.ravel())\n",
    "# accuracy 구하기\n",
    "g=rf_gini.score(x_test,y_test.values.ravel())\n",
    "e=rf_entropy.score(x_test,y_test.values.ravel())\n",
    "print('Accuracy of \"normal Random Forest\" by criterion = entropy: %.4f' % e)\n",
    "print('Accuracy of \"normal Random Forest\" by criterion = gini: %.4f' % g)\n",
    "\n",
    "randomForest_gini.append(g)\n",
    "randomForest_entropy.append(e)\n",
    "#############################  mean = 0, sd = 1 data################################\n",
    "\n",
    "x_train_ = scale_x_train\n",
    "x_test_ = scale_x_test\n",
    "\n",
    "\n",
    "# random forest 생성_gini\n",
    "rf_gini = RandomForestClassifier(random_state = 0).fit(x_train_,y_train.values.ravel())\n",
    "# random forest_entropy\n",
    "rf_entropy = RandomForestClassifier(criterion=\"entropy\" ,random_state = 0).fit(x_train_,y_train.values.ravel())\n",
    "# accuracy 구하기\n",
    "g=rf_gini.score(x_test_,y_test.values.ravel())\n",
    "e=rf_entropy.score(x_test_,y_test.values.ravel())\n",
    "print('Accuracy of \"scaled Random Forest\" by criterion = entropy: %.4f' % e)\n",
    "print('Accuracy of \"scaled Random Forest\" by criterion = gini: %.4f' % g)\n",
    "\n",
    "randomForest_gini.append(g)\n",
    "randomForest_entropy.append(e)\n",
    "######################  min max  [0,1]  data #############################\n",
    "\n",
    "x_train_ = min_max_x_train\n",
    "x_test_ = min_max_x_test\n",
    "\n",
    "# random forest 생성_gini\n",
    "rf_gini = RandomForestClassifier(random_state = 0).fit(x_train_,y_train.values.ravel())\n",
    "# random forest_entropy\n",
    "rf_entropy = RandomForestClassifier(criterion=\"entropy\" ,random_state = 0).fit(x_train_,y_train.values.ravel())\n",
    "# accuracy 구하기\n",
    "g=rf_gini.score(x_test_,y_test.values.ravel())\n",
    "e=rf_entropy.score(x_test_,y_test.values.ravel())\n",
    "print('Accuracy of \"min max Random Forest\" by criterion = entropy: %.4f' % e)\n",
    "print('Accuracy of \"min max Random Forest\" by criterion = gini: %.4f' % g)\n",
    "\n",
    "randomForest_gini.append(g)\n",
    "randomForest_entropy.append(e)\n",
    "#################### max abs [-1,1]  data  #####################################\n",
    "\n",
    "x_train_ = max_abs_x_train\n",
    "x_test_ = max_abs_x_test\n",
    "\n",
    "# random forest 생성_gini\n",
    "rf_gini = RandomForestClassifier(random_state = 0).fit(x_train_,y_train.values.ravel())\n",
    "# random forest_entropy\n",
    "rf_entropy = RandomForestClassifier(criterion=\"entropy\" ,random_state = 0).fit(x_train_,y_train.values.ravel())\n",
    "# accuracy 구하기\n",
    "g=rf_gini.score(x_test_,y_test.values.ravel())\n",
    "e=rf_entropy.score(x_test_,y_test.values.ravel())\n",
    "print('Accuracy of \"max abs Random Forest\" by criterion = entropy: %.4f' % e)\n",
    "print('Accuracy of \"max abs Random Forest\" by criterion = gini: %.4f' % g)\n",
    "\n",
    "randomForest_gini.append(g)\n",
    "randomForest_entropy.append(e)\n",
    "####################### normalization data #########################################\n",
    "\n",
    "x_train_ = normalization_x_train\n",
    "x_test_ = normalization_x_test\n",
    "\n",
    "# random forest 생성_gini\n",
    "rf_gini = RandomForestClassifier(random_state = 0).fit(x_train_,y_train.values.ravel())\n",
    "# random forest_entropy\n",
    "rf_entropy = RandomForestClassifier(criterion=\"entropy\" ,random_state = 0).fit(x_train_,y_train.values.ravel())\n",
    "# accuracy 구하기\n",
    "g=rf_gini.score(x_test_,y_test.values.ravel())\n",
    "e=rf_entropy.score(x_test_,y_test.values.ravel())\n",
    "print('Accuracy of \"normalization Random Forest\" by criterion = entropy: %.4f' % e)\n",
    "print('Accuracy of \"normalization Random Forest\" by criterion = gini: %.4f' % g)\n",
    "\n",
    "randomForest_gini.append(g)\n",
    "randomForest_entropy.append(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree_acc = pd.DataFrame()\n",
    "\n",
    "decisionTree_acc[\"gini\"] =decisionTree_gini\n",
    "decisionTree_acc[\"entropy\"] = decisionTree_entropy \n",
    "\n",
    "decisionTree_acc.index = [\"ori\",\"scale\",\"minmax\",\"max abs\",\"normalize\"]\n",
    "\n",
    "randomForest_acc = pd.DataFrame()\n",
    "\n",
    "randomForest_acc[\"gini\"] = randomForest_gini\n",
    "randomForest_acc[\"entropy\"]=randomForest_entropy\n",
    "randomForest_acc.index = [\"ori\",\"scale\",\"minmax\",\"max abs\",\"normalize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gini</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ori</th>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.935673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale</th>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.935673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minmax</th>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.935673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max abs</th>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.935673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normalize</th>\n",
       "      <td>0.935673</td>\n",
       "      <td>0.935673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               gini   entropy\n",
       "ori        0.912281  0.935673\n",
       "scale      0.912281  0.935673\n",
       "minmax     0.912281  0.935673\n",
       "max abs    0.912281  0.935673\n",
       "normalize  0.935673  0.935673"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisionTree_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gini</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ori</th>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale</th>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minmax</th>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max abs</th>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normalize</th>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.941520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               gini   entropy\n",
       "ori        0.964912  0.964912\n",
       "scale      0.964912  0.964912\n",
       "minmax     0.964912  0.964912\n",
       "max abs    0.964912  0.964912\n",
       "normalize  0.959064  0.941520"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-nearest neighbors classifier는 PCA를 활용하여 모델을 추가로 생성했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 1-nearest neighbors by normal data :  0.9181286549707602\n",
      "accuracy of 1-nearest neighbors by scaled data:  0.935672514619883\n",
      "accuracy of 1-nearest neighbors by min max data:  0.9298245614035088\n",
      "accuracy of 1-nearest neighbors by max abs  data:  0.9473684210526315\n",
      "accuracy of 1-nearest neighbors : by normalization  0.8596491228070176\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k1_ac = []\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k = 1\n",
    "####################################### normal ###################################\n",
    "knn1 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn1.fit(x_train,y_train)\n",
    "print(\"accuracy of 1-nearest neighbors by normal data : \",knn1.score(x_test,y_test))\n",
    "\n",
    "k1_ac.append(knn1.score(x_test,y_test))\n",
    "\n",
    "#############################  mean = 0, sd = 1 data################################\n",
    "\n",
    "x_train_ = scale_x_train\n",
    "x_test_ = scale_x_test\n",
    "knn1 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn1.fit(x_train_,y_train)\n",
    "print(\"accuracy of 1-nearest neighbors by scaled data: \",knn1.score(x_test_,y_test))\n",
    "\n",
    "k1_ac.append(knn1.score(x_test_,y_test))\n",
    "\n",
    "######################  min max  [0,1]  data #############################\n",
    "\n",
    "x_train_ = min_max_x_train\n",
    "x_test_ = min_max_x_test\n",
    "knn1 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn1.fit(x_train_,y_train)\n",
    "print(\"accuracy of 1-nearest neighbors by min max data: \",knn1.score(x_test_,y_test))\n",
    "\n",
    "k1_ac.append(knn1.score(x_test_,y_test))\n",
    "\n",
    "#################### max abs [-1,1]  data  #####################################\n",
    "\n",
    "x_train_ = max_abs_x_train\n",
    "x_test_ = max_abs_x_test\n",
    "knn1 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn1.fit(x_train_,y_train)\n",
    "print(\"accuracy of 1-nearest neighbors by max abs  data: \",knn1.score(x_test_,y_test))\n",
    "\n",
    "k1_ac.append(knn1.score(x_test_,y_test))\n",
    "\n",
    "####################### normalization data #########################################\n",
    "\n",
    "x_train_ = normalization_x_train\n",
    "x_test_ = normalization_x_test\n",
    "knn1 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn1.fit(x_train_,y_train)\n",
    "print(\"accuracy of 1-nearest neighbors : by normalization \",knn1.score(x_test_,y_test))\n",
    "\n",
    "k1_ac.append(knn1.score(x_test_,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9181286549707602,\n",
       " 0.935672514619883,\n",
       " 0.9298245614035088,\n",
       " 0.9473684210526315,\n",
       " 0.8596491228070176]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 1-nearest neighbors : by PCA 1 normal  0.8362573099415205\n",
      "accuracy of 1-nearest neighbors : by PCA 13 scaled 0.9415204678362573\n",
      "accuracy of 1-nearest neighbors : by PCA min max 12  0.9415204678362573\n",
      "accuracy of 1-nearest neighbors : by PCA max abs 12  0.9473684210526315\n",
      "accuracy of 1-nearest neighbors : by PCA 2  normalization 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "k=1\n",
    "######################### PCA ###############################\n",
    "k1_pca_ac = []\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_norm,y_ori,test_size = 0.3,random_state =0 )\n",
    "knn1 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn1.fit(x_train_,y_train)\n",
    "print(\"accuracy of 1-nearest neighbors : by PCA 1 normal \",knn1.score(x_test_,y_test_))\n",
    "k1_pca_ac.append(knn1.score(x_test_,y_test_))\n",
    "\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_scale,y_ori,test_size = 0.3,random_state =0 )\n",
    "knn1 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn1.fit(x_train_,y_train)\n",
    "print(\"accuracy of 1-nearest neighbors : by PCA 13 scaled\",knn1.score(x_test_,y_test_))\n",
    "k1_pca_ac.append(knn1.score(x_test_,y_test_))\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_minmax,y_ori,test_size = 0.3,random_state =0 )\n",
    "knn1 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn1.fit(x_train_,y_train)\n",
    "print(\"accuracy of 1-nearest neighbors : by PCA min max 12 \",knn1.score(x_test_,y_test_))\n",
    "k1_pca_ac.append(knn1.score(x_test_,y_test_))\n",
    "\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_maxabs,y_ori,test_size = 0.3,random_state =0 )\n",
    "knn1 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn1.fit(x_train_,y_train)\n",
    "print(\"accuracy of 1-nearest neighbors : by PCA max abs 12 \",knn1.score(x_test_,y_test_))\n",
    "k1_pca_ac.append(knn1.score(x_test_,y_test_))\n",
    "\n",
    "x_train_, x_test_ ,y_train_,y_test_= train_test_split(pca_normal,y_ori,test_size = 0.3,random_state =0 )\n",
    "knn1 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn1.fit(x_train_,y_train)\n",
    "print(\"accuracy of 1-nearest neighbors : by PCA 2  normalization\",knn1.score(x_test_,y_test_))\n",
    "k1_pca_ac.append(knn1.score(x_test_,y_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 2-nearest neighbors by normal data :  0.9181286549707602\n",
      "accuracy of 2-nearest neighbors by scaled data:  0.935672514619883\n",
      "accuracy of 2-nearest neighbors by min max data:  0.9298245614035088\n",
      "accuracy of 2-nearest neighbors by max abs  data:  0.9473684210526315\n",
      "accuracy of 2-nearest neighbors : by normalization  0.8596491228070176\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "k2_ac = []\n",
    "\n",
    "####################################### normal ###################################\n",
    "knn2 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn2.fit(x_train,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors by normal data : \",knn2.score(x_test,y_test))\n",
    "k2_ac.append(knn2.score(x_test,y_test))\n",
    "\n",
    "#############################  mean = 0, sd = 1 data################################\n",
    "\n",
    "x_train_ = scale_x_train\n",
    "x_test_ = scale_x_test\n",
    "knn2 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn2.fit(x_train_,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors by scaled data: \",knn2.score(x_test_,y_test))\n",
    "k2_ac.append(knn2.score(x_test_,y_test))\n",
    "\n",
    "######################  min max  [0,1]  data #############################\n",
    "\n",
    "x_train_ = min_max_x_train\n",
    "x_test_ = min_max_x_test\n",
    "knn2 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn2.fit(x_train_,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors by min max data: \",knn2.score(x_test_,y_test))\n",
    "k2_ac.append(knn2.score(x_test_,y_test))\n",
    "\n",
    "#################### max abs [-1,1]  data  #####################################\n",
    "\n",
    "x_train_ = max_abs_x_train\n",
    "x_test_ = max_abs_x_test\n",
    "knn2 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn2.fit(x_train_,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors by max abs  data: \",knn2.score(x_test_,y_test))\n",
    "k2_ac.append(knn2.score(x_test_,y_test))\n",
    "####################### normalization data #########################################\n",
    "\n",
    "x_train_ = normalization_x_train\n",
    "x_test_ = normalization_x_test\n",
    "knn2 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn2.fit(x_train_,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors : by normalization \",knn2.score(x_test_,y_test))\n",
    "k2_ac.append(knn2.score(x_test_,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 2-nearest neighbors : by PCA 1 normal  0.8362573099415205\n",
      "accuracy of 2-nearest neighbors : by PCA 9 scaled 0.9415204678362573\n",
      "accuracy of 2-nearest neighbors : by PCA min max 9  0.9415204678362573\n",
      "accuracy of 2-nearest neighbors : by PCA max abs 8  0.9473684210526315\n",
      "accuracy of 2-nearest neighbors : by PCA 2  normalization 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "k=2\n",
    "k2_pca_ac = []\n",
    "######################### PCA ###############################\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_norm,y_ori,test_size = 0.3,random_state =0 )\n",
    "knn2 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn2.fit(x_train_,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors : by PCA 1 normal \",knn2.score(x_test_,y_test_))\n",
    "k2_pca_ac.append(knn2.score(x_test_,y_test_))\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_scale,y_ori,test_size = 0.3,random_state =0 )\n",
    "knn2 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn2.fit(x_train_,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors : by PCA 9 scaled\",knn2.score(x_test_,y_test_))\n",
    "k2_pca_ac.append(knn2.score(x_test_,y_test_))\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_minmax,y_ori,test_size = 0.3,random_state =0 )\n",
    "knn2 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn2.fit(x_train_,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors : by PCA min max 9 \",knn2.score(x_test_,y_test_))\n",
    "k2_pca_ac.append(knn2.score(x_test_,y_test_))\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_maxabs,y_ori,test_size = 0.3,random_state =0 )\n",
    "knn2 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn2.fit(x_train_,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors : by PCA max abs 8 \",knn2.score(x_test_,y_test_))\n",
    "k2_pca_ac.append(knn2.score(x_test_,y_test_))\n",
    "\n",
    "x_train_, x_test_ ,y_train_,y_test_= train_test_split(pca_normal,y_ori,test_size = 0.3,random_state =0 )\n",
    "knn2 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn2.fit(x_train_,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors : by PCA 2  normalization\",knn2.score(x_test_,y_test_))\n",
    "k2_pca_ac.append(knn2.score(x_test_,y_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 3-nearest neighbors by normal data :  0.9239766081871345\n",
      "accuracy of 3-nearest neighbors by scaled data:  0.9473684210526315\n",
      "accuracy of 3-nearest neighbors by min max data:  0.9590643274853801\n",
      "accuracy of 3-nearest neighbors by max abs  data:  0.9415204678362573\n",
      "accuracy of 3-nearest neighbors : by normalization  0.8830409356725146\n"
     ]
    }
   ],
   "source": [
    "k =3\n",
    "k3_ac = []\n",
    "\n",
    "####################################### normal ###################################\n",
    "knn3 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn3.fit(x_train,y_train)\n",
    "print(\"accuracy of 3-nearest neighbors by normal data : \",knn3.score(x_test,y_test))\n",
    "a = knn3.score(x_test,y_test)\n",
    "k3_ac.append(a)\n",
    "\n",
    "#############################  mean = 0, sd = 1 data################################\n",
    "\n",
    "x_train_ = scale_x_train\n",
    "x_test_ = scale_x_test\n",
    "knn3 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn3.fit(x_train_,y_train)\n",
    "print(\"accuracy of 3-nearest neighbors by scaled data: \",knn3.score(x_test_,y_test))\n",
    "a = knn3.score(x_test_,y_test)\n",
    "k3_ac.append(a)\n",
    "\n",
    "######################  min max  [0,1]  data #############################\n",
    "\n",
    "x_train_ = min_max_x_train\n",
    "x_test_ = min_max_x_test\n",
    "knn3 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn3.fit(x_train_,y_train)\n",
    "print(\"accuracy of 3-nearest neighbors by min max data: \",knn3.score(x_test_,y_test))\n",
    "a = knn3.score(x_test_,y_test)\n",
    "k3_ac.append(a)\n",
    "\n",
    "#################### max abs [-1,1]  data  #####################################\n",
    "\n",
    "x_train_ = max_abs_x_train\n",
    "x_test_ = max_abs_x_test\n",
    "knn3 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn3.fit(x_train_,y_train)\n",
    "print(\"accuracy of 3-nearest neighbors by max abs  data: \",knn3.score(x_test_,y_test))\n",
    "a = knn3.score(x_test_,y_test)\n",
    "k3_ac.append(a)\n",
    "\n",
    "####################### normalization data #########################################\n",
    "\n",
    "x_train_ = normalization_x_train\n",
    "x_test_ = normalization_x_test\n",
    "knn3 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn3.fit(x_train_,y_train)\n",
    "print(\"accuracy of 3-nearest neighbors : by normalization \",knn3.score(x_test_,y_test))\n",
    "a = knn3.score(x_test_,y_test)\n",
    "k3_ac.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 3-nearest neighbors : by PCA 1 normal  0.8888888888888888\n",
      "accuracy of 3-nearest neighbors : by PCA 9 scaled 0.9590643274853801\n",
      "accuracy of 2-nearest neighbors : by PCA min max 9  0.9590643274853801\n",
      "accuracy of 2-nearest neighbors : by PCA max abs 8  0.9473684210526315\n",
      "accuracy of 3-nearest neighbors : by PCA 2  normalization 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "k=3\n",
    "k3_pca_ac = []\n",
    "\n",
    "######################### PCA ###############################\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_norm,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn3.fit(x_train_,y_train)\n",
    "print(\"accuracy of 3-nearest neighbors : by PCA 1 normal \",knn3.score(x_test_,y_test_))\n",
    "pc = knn3.score(x_test_,y_test_)\n",
    "k3_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_scale,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn3.fit(x_train_,y_train)\n",
    "print(\"accuracy of 3-nearest neighbors : by PCA 9 scaled\",knn3.score(x_test_,y_test_))\n",
    "pc = knn3.score(x_test_,y_test_)\n",
    "k3_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_minmax,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn3.fit(x_train_,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors : by PCA min max 9 \",knn3.score(x_test_,y_test_))\n",
    "pc = knn3.score(x_test_,y_test_)\n",
    "k3_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_maxabs,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn3.fit(x_train_,y_train)\n",
    "print(\"accuracy of 2-nearest neighbors : by PCA max abs 8 \",knn3.score(x_test_,y_test_))\n",
    "pc = knn3.score(x_test_,y_test_)\n",
    "k3_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_ ,y_train_,y_test_= train_test_split(pca_normal,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn3.fit(x_train_,y_train)\n",
    "print(\"accuracy of 3-nearest neighbors : by PCA 2  normalization\",knn3.score(x_test_,y_test_))\n",
    "pc = knn3.score(x_test_,y_test_)\n",
    "k3_pca_ac.append(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 4-nearest neighbors by normal data :  0.9239766081871345\n",
      "accuracy of 4-nearest neighbors by scaled data:  0.9532163742690059\n",
      "accuracy of 4-nearest neighbors by min max data:  0.9590643274853801\n",
      "accuracy of 4-nearest neighbors by max abs  data:  0.9473684210526315\n",
      "accuracy of 4-nearest neighbors : by normalization  0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "k =4\n",
    "k4_ac = []\n",
    "####################################### normal ###################################\n",
    "knn4 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn4.fit(x_train,y_train)\n",
    "print(\"accuracy of 4-nearest neighbors by normal data : \",knn4.score(x_test,y_test))\n",
    "a = knn4.score(x_test,y_test)\n",
    "k4_ac.append(a)\n",
    "\n",
    "#############################  mean = 0, sd = 1 data################################\n",
    "\n",
    "x_train_ = scale_x_train\n",
    "x_test_ = scale_x_test\n",
    "knn4 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn4.fit(x_train_,y_train)\n",
    "print(\"accuracy of 4-nearest neighbors by scaled data: \",knn4.score(x_test_,y_test))\n",
    "a = knn4.score(x_test_,y_test)\n",
    "k4_ac.append(a)\n",
    "\n",
    "######################  min max  [0,1]  data #############################\n",
    "\n",
    "x_train_ = min_max_x_train\n",
    "x_test_ = min_max_x_test\n",
    "knn4 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn4.fit(x_train_,y_train)\n",
    "print(\"accuracy of 4-nearest neighbors by min max data: \",knn4.score(x_test_,y_test))\n",
    "a = knn4.score(x_test_,y_test)\n",
    "k4_ac.append(a)\n",
    "\n",
    "#################### max abs [-1,1]  data  #####################################\n",
    "\n",
    "x_train_ = max_abs_x_train\n",
    "x_test_ = max_abs_x_test\n",
    "knn4 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn4.fit(x_train_,y_train)\n",
    "print(\"accuracy of 4-nearest neighbors by max abs  data: \",knn4.score(x_test_,y_test))\n",
    "a = knn4.score(x_test_,y_test)\n",
    "k4_ac.append(a)\n",
    "\n",
    "####################### normalization data #########################################\n",
    "\n",
    "x_train_ = normalization_x_train\n",
    "x_test_ = normalization_x_test\n",
    "knn4 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn4.fit(x_train_,y_train)\n",
    "print(\"accuracy of 4-nearest neighbors : by normalization \",knn4.score(x_test_,y_test))\n",
    "a = knn4.score(x_test_,y_test)\n",
    "k4_ac.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 4-nearest neighbors : by PCA 1 normal  0.8947368421052632\n",
      "accuracy of 4-nearest neighbors : by PCA 9 scaled 0.9532163742690059\n",
      "accuracy of 4-nearest neighbors : by PCA min max 9  0.9590643274853801\n",
      "accuracy of 4-nearest neighbors : by PCA max abs 8  0.9532163742690059\n",
      "accuracy of 4-nearest neighbors : by PCA 2  normalization 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "k=4\n",
    "k4_pca_ac = []\n",
    "######################### PCA ###############################\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_norm,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn4 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn4.fit(x_train_,y_train)\n",
    "print(\"accuracy of 4-nearest neighbors : by PCA 1 normal \",knn4.score(x_test_,y_test_))\n",
    "pc = knn4.score(x_test_,y_test_)\n",
    "k4_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_scale,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn4 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn4.fit(x_train_,y_train)\n",
    "print(\"accuracy of 4-nearest neighbors : by PCA 9 scaled\",knn4.score(x_test_,y_test_))\n",
    "pc = knn4.score(x_test_,y_test_)\n",
    "k4_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_minmax,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn4 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn4.fit(x_train_,y_train)\n",
    "print(\"accuracy of 4-nearest neighbors : by PCA min max 9 \",knn4.score(x_test_,y_test_))\n",
    "pc = knn4.score(x_test_,y_test_)\n",
    "k4_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_maxabs,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn4 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn4.fit(x_train_,y_train)\n",
    "print(\"accuracy of 4-nearest neighbors : by PCA max abs 8 \",knn4.score(x_test_,y_test_))\n",
    "pc = knn4.score(x_test_,y_test_)\n",
    "k4_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_ ,y_train_,y_test_= train_test_split(pca_normal,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn4 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn4.fit(x_train_,y_train)\n",
    "print(\"accuracy of 4-nearest neighbors : by PCA 2  normalization\",knn4.score(x_test_,y_test_))\n",
    "pc = knn4.score(x_test_,y_test_)\n",
    "k4_pca_ac.append(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 5-nearest neighbors by normal data :  0.9473684210526315\n",
      "accuracy of 5-nearest neighbors by scaled data:  0.9590643274853801\n",
      "accuracy of 5-nearest neighbors by min max data:  0.9707602339181286\n",
      "accuracy of 5-nearest neighbors by max abs  data:  0.9532163742690059\n",
      "accuracy of 5-nearest neighbors : by normalization  0.9239766081871345\n"
     ]
    }
   ],
   "source": [
    "k =5\n",
    "k5_ac = []\n",
    "####################################### normal ###################################\n",
    "knn5 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn5.fit(x_train,y_train)\n",
    "print(\"accuracy of 5-nearest neighbors by normal data : \",knn5.score(x_test,y_test))\n",
    "a = knn5.score(x_test,y_test)\n",
    "k5_ac.append(a)\n",
    "\n",
    "#############################  mean = 0, sd = 1 data################################\n",
    "\n",
    "x_train_ = scale_x_train\n",
    "x_test_ = scale_x_test\n",
    "knn5 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn5.fit(x_train_,y_train)\n",
    "print(\"accuracy of 5-nearest neighbors by scaled data: \",knn5.score(x_test_,y_test))\n",
    "a = knn5.score(x_test_,y_test)\n",
    "k5_ac.append(a)\n",
    "\n",
    "######################  min max  [0,1]  data #############################\n",
    "\n",
    "x_train_ = min_max_x_train\n",
    "x_test_ = min_max_x_test\n",
    "knn5 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn5.fit(x_train_,y_train)\n",
    "print(\"accuracy of 5-nearest neighbors by min max data: \",knn5.score(x_test_,y_test))\n",
    "a = knn5.score(x_test_,y_test)\n",
    "k5_ac.append(a)\n",
    "\n",
    "#################### max abs [-1,1]  data  #####################################\n",
    "\n",
    "x_train_ = max_abs_x_train\n",
    "x_test_ = max_abs_x_test\n",
    "knn5 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn5.fit(x_train_,y_train)\n",
    "print(\"accuracy of 5-nearest neighbors by max abs  data: \",knn5.score(x_test_,y_test))\n",
    "a = knn5.score(x_test_,y_test)\n",
    "k5_ac.append(a)\n",
    "\n",
    "####################### normalization data #########################################\n",
    "\n",
    "x_train_ = normalization_x_train\n",
    "x_test_ = normalization_x_test\n",
    "knn5 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn5.fit(x_train_,y_train)\n",
    "print(\"accuracy of 5-nearest neighbors : by normalization \",knn5.score(x_test_,y_test))\n",
    "a = knn5.score(x_test_,y_test)\n",
    "k5_ac.append(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 5-nearest neighbors : by PCA 1 normal  0.9064327485380117\n",
      "accuracy of 5-nearest neighbors : by PCA 9 scaled 0.9590643274853801\n",
      "accuracy of 5-nearest neighbors : by PCA min max 9  0.9649122807017544\n",
      "accuracy of 5-nearest neighbors : by PCA max abs 8  0.9532163742690059\n",
      "accuracy of 5-nearest neighbors : by PCA 2  normalization 0.9064327485380117\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "k5_pca_ac = []\n",
    "######################### PCA ###############################\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_norm,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn5 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn5.fit(x_train_,y_train)\n",
    "print(\"accuracy of 5-nearest neighbors : by PCA 1 normal \",knn5.score(x_test_,y_test_))\n",
    "pc = knn5.score(x_test_,y_test_)\n",
    "k5_pca_ac.append(pc)\n",
    "\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_scale,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn5 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn5.fit(x_train_,y_train)\n",
    "print(\"accuracy of 5-nearest neighbors : by PCA 9 scaled\",knn5.score(x_test_,y_test_))\n",
    "pc = knn5.score(x_test_,y_test_)\n",
    "k5_pca_ac.append(pc)\n",
    "\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_minmax,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn5 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn5.fit(x_train_,y_train)\n",
    "print(\"accuracy of 5-nearest neighbors : by PCA min max 9 \",knn5.score(x_test_,y_test_))\n",
    "pc = knn5.score(x_test_,y_test_)\n",
    "k5_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_maxabs,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn5 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn5.fit(x_train_,y_train)\n",
    "print(\"accuracy of 5-nearest neighbors : by PCA max abs 8 \",knn5.score(x_test_,y_test_))\n",
    "pc = knn5.score(x_test_,y_test_)\n",
    "k5_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_ ,y_train_,y_test_= train_test_split(pca_normal,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn5 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn5.fit(x_train_,y_train)\n",
    "print(\"accuracy of 5-nearest neighbors : by PCA 2  normalization\",knn5.score(x_test_,y_test_))\n",
    "pc = knn5.score(x_test_,y_test_)\n",
    "k5_pca_ac.append(pc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 6-nearest neighbors by normal data :  0.9415204678362573\n",
      "accuracy of 6-nearest neighbors by scaled data:  0.9649122807017544\n",
      "accuracy of 6-nearest neighbors by min max data:  0.9707602339181286\n",
      "accuracy of 6-nearest neighbors by max abs  data:  0.9532163742690059\n",
      "accuracy of 6-nearest neighbors : by normalization  0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "k =6\n",
    "k6_ac = []\n",
    "####################################### normal ###################################\n",
    "knn6 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn6.fit(x_train,y_train)\n",
    "print(\"accuracy of 6-nearest neighbors by normal data : \",knn6.score(x_test,y_test))\n",
    "a = knn6.score(x_test,y_test)\n",
    "k6_ac.append(a)\n",
    "\n",
    "\n",
    "#############################  mean = 0, sd = 1 data################################\n",
    "\n",
    "x_train_ = scale_x_train\n",
    "x_test_ = scale_x_test\n",
    "knn6 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn6.fit(x_train_,y_train)\n",
    "print(\"accuracy of 6-nearest neighbors by scaled data: \",knn6.score(x_test_,y_test))\n",
    "a = knn6.score(x_test_,y_test)\n",
    "k6_ac.append(a)\n",
    "\n",
    "######################  min max  [0,1]  data #############################\n",
    "\n",
    "x_train_ = min_max_x_train\n",
    "x_test_ = min_max_x_test\n",
    "knn6 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn6.fit(x_train_,y_train)\n",
    "print(\"accuracy of 6-nearest neighbors by min max data: \",knn6.score(x_test_,y_test))\n",
    "a = knn6.score(x_test_,y_test)\n",
    "k6_ac.append(a)\n",
    "\n",
    "#################### max abs [-1,1]  data  #####################################\n",
    "\n",
    "x_train_ = max_abs_x_train\n",
    "x_test_ = max_abs_x_test\n",
    "knn6 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn6.fit(x_train_,y_train)\n",
    "print(\"accuracy of 6-nearest neighbors by max abs  data: \",knn6.score(x_test_,y_test))\n",
    "a = knn6.score(x_test_,y_test)\n",
    "k6_ac.append(a)\n",
    "\n",
    "####################### normalization data #########################################\n",
    "\n",
    "x_train_ = normalization_x_train\n",
    "x_test_ = normalization_x_test\n",
    "knn6 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn6.fit(x_train_,y_train)\n",
    "print(\"accuracy of 6-nearest neighbors : by normalization \",knn6.score(x_test_,y_test))\n",
    "a = knn6.score(x_test_,y_test)\n",
    "k6_ac.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of 6-nearest neighbors : by PCA 1 normal  0.9005847953216374\n",
      "accuracy of 6-nearest neighbors : by PCA 9 scaled 0.9590643274853801\n",
      "accuracy of 6-nearest neighbors : by PCA min max 9  0.9707602339181286\n",
      "accuracy of 6-nearest neighbors : by PCA max abs 8  0.9473684210526315\n",
      "accuracy of 6-nearest neighbors : by PCA 2  normalization 0.9064327485380117\n"
     ]
    }
   ],
   "source": [
    "k=6\n",
    "k6_pca_ac =[]\n",
    "######################### PCA ###############################\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_norm,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn6 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn6.fit(x_train_,y_train_)\n",
    "print(\"accuracy of 6-nearest neighbors : by PCA 1 normal \",knn6.score(x_test_,y_test_))\n",
    "pc = knn6.score(x_test_,y_test_)\n",
    "k6_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_scale,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn6 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn6.fit(x_train_,y_train_)\n",
    "print(\"accuracy of 6-nearest neighbors : by PCA 9 scaled\",knn6.score(x_test_,y_test_))\n",
    "pc = knn6.score(x_test_,y_test_)\n",
    "k6_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_minmax,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn6 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn6.fit(x_train_,y_train_)\n",
    "print(\"accuracy of 6-nearest neighbors : by PCA min max 9 \",knn6.score(x_test_,y_test_))\n",
    "pc = knn6.score(x_test_,y_test_)\n",
    "k6_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(pca_maxabs,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn6 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn6.fit(x_train_,y_train_)\n",
    "print(\"accuracy of 6-nearest neighbors : by PCA max abs 8 \",knn6.score(x_test_,y_test_))\n",
    "pc = knn6.score(x_test_,y_test_)\n",
    "k6_pca_ac.append(pc)\n",
    "\n",
    "x_train_, x_test_ ,y_train_,y_test_= train_test_split(pca_normal,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "knn6 = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "knn6.fit(x_train_,y_train_)\n",
    "print(\"accuracy of 6-nearest neighbors : by PCA 2  normalization\",knn6.score(x_test_,y_test_))\n",
    "pc = knn6.score(x_test_,y_test_)\n",
    "k6_pca_ac.append(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_table_knn = pd.DataFrame()\n",
    "accuracy_table_knn[\" k= 1\"] = k1_ac\n",
    "accuracy_table_knn[\" k= 2\"] = k2_ac\n",
    "accuracy_table_knn[\" k= 3\"] = k3_ac\n",
    "accuracy_table_knn[\" k= 4\"] = k4_ac\n",
    "accuracy_table_knn[\" k= 5\"] = k5_ac\n",
    "accuracy_table_knn[\" k= 6\"] = k6_ac\n",
    "accuracy_table_knn.index = [\"ori\",\"scale\",\"minmax\",\"max abs\",\"normalize\"]\n",
    "\n",
    "accuracy_table_knn_pca = pd.DataFrame()\n",
    "accuracy_table_knn_pca[\" k= 1\"] = k1_pca_ac\n",
    "accuracy_table_knn_pca[\" k= 2\"] = k2_pca_ac\n",
    "accuracy_table_knn_pca[\" k= 3\"] = k3_pca_ac\n",
    "accuracy_table_knn_pca[\" k= 4\"] = k4_pca_ac\n",
    "accuracy_table_knn_pca[\" k= 5\"] = k5_pca_ac\n",
    "accuracy_table_knn_pca[\" k= 6\"] = k6_pca_ac\n",
    "accuracy_table_knn_pca.index = [\"ori\",\"scale\",\"minmax\",\"max abs\",\"normalize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k= 1</th>\n",
       "      <th>k= 2</th>\n",
       "      <th>k= 3</th>\n",
       "      <th>k= 4</th>\n",
       "      <th>k= 5</th>\n",
       "      <th>k= 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ori</th>\n",
       "      <td>0.918129</td>\n",
       "      <td>0.918129</td>\n",
       "      <td>0.923977</td>\n",
       "      <td>0.923977</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.941520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale</th>\n",
       "      <td>0.935673</td>\n",
       "      <td>0.935673</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minmax</th>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.970760</td>\n",
       "      <td>0.970760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max abs</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.941520</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.953216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normalize</th>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.883041</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.923977</td>\n",
       "      <td>0.929825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               k= 1      k= 2      k= 3      k= 4      k= 5      k= 6\n",
       "ori        0.918129  0.918129  0.923977  0.923977  0.947368  0.941520\n",
       "scale      0.935673  0.935673  0.947368  0.953216  0.959064  0.964912\n",
       "minmax     0.929825  0.929825  0.959064  0.959064  0.970760  0.970760\n",
       "max abs    0.947368  0.947368  0.941520  0.947368  0.953216  0.953216\n",
       "normalize  0.859649  0.859649  0.883041  0.888889  0.923977  0.929825"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_table_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k= 1</th>\n",
       "      <th>k= 2</th>\n",
       "      <th>k= 3</th>\n",
       "      <th>k= 4</th>\n",
       "      <th>k= 5</th>\n",
       "      <th>k= 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ori</th>\n",
       "      <td>0.836257</td>\n",
       "      <td>0.836257</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.906433</td>\n",
       "      <td>0.900585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale</th>\n",
       "      <td>0.941520</td>\n",
       "      <td>0.941520</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.959064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minmax</th>\n",
       "      <td>0.941520</td>\n",
       "      <td>0.941520</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.970760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max abs</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normalize</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.906433</td>\n",
       "      <td>0.906433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               k= 1      k= 2      k= 3      k= 4      k= 5      k= 6\n",
       "ori        0.836257  0.836257  0.888889  0.894737  0.906433  0.900585\n",
       "scale      0.941520  0.941520  0.959064  0.953216  0.959064  0.959064\n",
       "minmax     0.941520  0.941520  0.959064  0.959064  0.964912  0.970760\n",
       "max abs    0.947368  0.947368  0.947368  0.953216  0.953216  0.947368\n",
       "normalize  0.888889  0.888889  0.894737  0.894737  0.906433  0.906433"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_table_knn_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee Joo Ye\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9064327485380117"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perceptron\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ = train_test_split(x_ori,y_ori,test_size = 0.3,random_state =0 )\n",
    "perceptron = Perceptron(random_state=0).fit(x_train_, y_train_.values.ravel())\n",
    "\n",
    "perceptron.score(x_test_, y_test_.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "activation=('identity', 'logistic', 'tanh', 'relu')\n",
    "solver = ['lbfgs', 'sgd','adam']\n",
    "\n",
    "# solver\n",
    "\n",
    "mlp_acccuracy = pd.DataFrame()\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ =  train_test_split(x_ori,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "for i in solver:\n",
    "    mlp_relu = MLPClassifier(solver = i, random_state=1, max_iter=10000).fit(x_train_, y_train)\n",
    "    mlp_tanh = MLPClassifier(solver = i,random_state=1,activation='tanh', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    mlp_i = MLPClassifier(solver = i,random_state=1,activation='identity', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    mlp_l = MLPClassifier(solver = i,random_state=1,activation='logistic', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    \n",
    "    mlp_acccuracy[i] = [mlp_i.score(x_test_, y_test_.values.ravel()),mlp_l.score(x_test_, y_test_.values.ravel()),mlp_tanh.score(x_test_, y_test_.values.ravel()),mlp_relu.score(x_test_, y_test_.values.ravel()) ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_acccuracy.index=activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbfgs</th>\n",
       "      <th>sgd</th>\n",
       "      <th>adam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>identity</th>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic</th>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.918129</td>\n",
       "      <td>0.941520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanh</th>\n",
       "      <td>0.970760</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.912281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relu</th>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.707602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             lbfgs       sgd      adam\n",
       "identity  0.953216  0.368421  0.894737\n",
       "logistic  0.964912  0.918129  0.941520\n",
       "tanh      0.970760  0.894737  0.912281\n",
       "relu      0.964912  0.631579  0.707602"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_acccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             lbfgs       sgd      adam\n",
      "identity  0.941520  0.970760  0.959064\n",
      "logistic  0.947368  0.953216  0.976608\n",
      "tanh      0.953216  0.964912  0.964912\n",
      "relu      0.578947  0.298246  0.497076\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "activation=('identity', 'logistic', 'tanh', 'relu')\n",
    "solver = ['lbfgs', 'sgd','adam']\n",
    "\n",
    "# solver\n",
    "\n",
    "mlp_scale_acccuracy = pd.DataFrame()\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ =  train_test_split(scale_x,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "for i in solver:\n",
    "    mlp_relu = MLPClassifier(solver = i, random_state=1, max_iter=10000).fit(x_train, y_train)\n",
    "    mlp_tanh = MLPClassifier(solver = i,random_state=1,activation='tanh', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    mlp_i = MLPClassifier(solver = i,random_state=1,activation='identity', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    mlp_l = MLPClassifier(solver = i,random_state=1,activation='logistic', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    \n",
    "    mlp_scale_acccuracy[i] = [mlp_i.score(x_test_, y_test_.values.ravel()),mlp_l.score(x_test_, y_test_.values.ravel()),mlp_tanh.score(x_test_, y_test_.values.ravel()),mlp_relu.score(x_test_, y_test_.values.ravel()) ]\n",
    "mlp_scale_acccuracy.index=activation   \n",
    "print(mlp_scale_acccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             lbfgs       sgd      adam\n",
      "identity  0.941520  0.929825  0.970760\n",
      "logistic  0.953216  0.631579  0.982456\n",
      "tanh      0.953216  0.929825  0.970760\n",
      "relu      0.654971  0.216374  0.631579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "activation=('identity', 'logistic', 'tanh', 'relu')\n",
    "solver = ['lbfgs', 'sgd','adam']\n",
    "\n",
    "# solver\n",
    "\n",
    "mlp_minmax_acccuracy = pd.DataFrame()\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ =  train_test_split(min_max_x,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "for i in solver:\n",
    "    mlp_relu = MLPClassifier(solver = i, random_state=1, max_iter=10000).fit(x_train, y_train)\n",
    "    mlp_tanh = MLPClassifier(solver = i,random_state=1,activation='tanh', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    mlp_i = MLPClassifier(solver = i,random_state=1,activation='identity', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    mlp_l = MLPClassifier(solver = i,random_state=1,activation='logistic', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    \n",
    "    mlp_minmax_acccuracy[i] = [mlp_i.score(x_test_, y_test_.values.ravel()),mlp_l.score(x_test_, y_test_.values.ravel()),mlp_tanh.score(x_test_, y_test_.values.ravel()),mlp_relu.score(x_test_, y_test_.values.ravel()) ]\n",
    "mlp_minmax_acccuracy.index=activation   \n",
    "print(mlp_minmax_acccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             lbfgs       sgd      adam\n",
      "identity  0.947368  0.918129  0.959064\n",
      "logistic  0.959064  0.631579  0.959064\n",
      "tanh      0.947368  0.918129  0.959064\n",
      "relu      0.654971  0.239766  0.631579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "activation=('identity', 'logistic', 'tanh', 'relu')\n",
    "solver = ['lbfgs', 'sgd','adam']\n",
    "\n",
    "# solver\n",
    "\n",
    "mlp_maxabs_acccuracy = pd.DataFrame()\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ =  train_test_split(max_abs_x,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "for i in solver:\n",
    "    mlp_relu = MLPClassifier(solver = i, random_state=1, max_iter=10000).fit(x_train, y_train)\n",
    "    mlp_tanh = MLPClassifier(solver = i,random_state=1,activation='tanh', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    mlp_i = MLPClassifier(solver = i,random_state=1,activation='identity', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    mlp_l = MLPClassifier(solver = i,random_state=1,activation='logistic', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    \n",
    "    mlp_maxabs_acccuracy[i] = [mlp_i.score(x_test_, y_test_.values.ravel()),mlp_l.score(x_test_, y_test_.values.ravel()),mlp_tanh.score(x_test_, y_test_.values.ravel()),mlp_relu.score(x_test_, y_test_.values.ravel()) ]\n",
    "mlp_maxabs_acccuracy.index=activation   \n",
    "print(mlp_maxabs_acccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             lbfgs       sgd      adam\n",
      "identity  0.941520  0.631579  0.953216\n",
      "logistic  0.935673  0.631579  0.631579\n",
      "tanh      0.929825  0.631579  0.953216\n",
      "relu      0.631579  0.631579  0.631579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "activation=('identity', 'logistic', 'tanh', 'relu')\n",
    "solver = ['lbfgs', 'sgd','adam']\n",
    "\n",
    "# solver\n",
    "\n",
    "mlp_noramlize_acccuracy = pd.DataFrame()\n",
    "\n",
    "x_train_, x_test_,y_train_,y_test_ =  train_test_split(normalization_x,y_ori,test_size = 0.3,random_state =0 )\n",
    "\n",
    "for i in solver:\n",
    "    mlp_relu = MLPClassifier(solver = i, random_state=1, max_iter=10000).fit(x_train, y_train)\n",
    "    mlp_tanh = MLPClassifier(solver = i,random_state=1,activation='tanh', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    mlp_i = MLPClassifier(solver = i,random_state=1,activation='identity', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    mlp_l = MLPClassifier(solver = i,random_state=1,activation='logistic', max_iter=10000).fit(x_train_, y_train_.values.ravel())\n",
    "    \n",
    "    mlp_noramlize_acccuracy[i] = [mlp_i.score(x_test_, y_test_.values.ravel()),mlp_l.score(x_test_, y_test_.values.ravel()),mlp_tanh.score(x_test_, y_test_.values.ravel()),mlp_relu.score(x_test_, y_test_.values.ravel()) ]\n",
    "mlp_noramlize_acccuracy.index=activation   \n",
    "print(mlp_noramlize_acccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
